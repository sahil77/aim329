{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d6e9f4",
   "metadata": {},
   "source": [
    "# AIM329 - Build a chat assistant with Amazon Bedrock\n",
    "\n",
    "Welcome to session AIM329 of AWS re:Invent 2023 - Build a chat assistant with Amazon Bedrock.\n",
    "\n",
    "This notebook will walk you through the process of building a chat assistant using a Large Language Model (LLM) hosted on [Amazon Bedrock](https://aws.amazon.com/bedrock/). We will use the [Retrieval Augment Generation (RAG)](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html) architecture with the [Amazon Titan Embeddings](https://aws.amazon.com/about-aws/whats-new/2023/09/amazon-titan-embeddings-generally-available/) model to convert raw text to vectors and [Amazon OpenSearch Serverless](https://aws.amazon.com/opensearch-service/features/serverless/) to store the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0111293",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "    <ul>\n",
    "        <li>This notebook should only be run from within an <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html\">Amazon SageMaker Notebook instance</a> or within an <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks.html\">Amazon SageMaker Studio Notebook</a>.</li>\n",
    "        <li>At the time of this writing, the most relevant latest version of the Kernel for this notebook was <i>conda_python3</i>.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e88d20",
   "metadata": {},
   "source": [
    "**Table of Contents:**\n",
    "\n",
    "1. [Complete prerequisites](#Complete%20prerequisites)\n",
    "\n",
    "    1. [Check and configure access to the Internet](#Check%20and%20configure%20access%20to%20the%20Internet)\n",
    "\n",
    "    2. [Check and upgrade required software versions](#Check%20and%20upgrade%20required%20software%20versions)\n",
    "    \n",
    "    3. [Create an Amazon OpenSearch Serverless collection](#Create%20an%20Amazon%20OpenSearch%20Serverless%20collection)\n",
    "    \n",
    "    4. [Enable model access in Amazon Bedrock](#Enable%20model%20access%20in%20Amazon%20Bedrock)\n",
    "    \n",
    "    5. [Check and configure security permissions](#Check%20and%20configure%20security%20permissions)\n",
    "\n",
    "    6. [Organize imports](#Organize%20imports)\n",
    "    \n",
    "    7. [Create common objects](#Create%20common%20objects)\n",
    "    \n",
    "    8. [Create an index in the Amazon OpenSearch Serverless collection](#Create%20index%20in%20collection)\n",
    "    \n",
    " 2. [Build the chat assistant](#Build%20the%20chat%20assistant)\n",
    "\n",
    "    1. [Architecture](#Architecture)\n",
    "    \n",
    "    2. [Step 0a: Prepare to load data into the vector database](#Step0a)\n",
    "    \n",
    "    3. [Step 0b and 0c: Create the embeddings](#Step0band0c)\n",
    "    \n",
    "    4. [Step 0d: Store the embeddings in the vector database](#Step0d)\n",
    "    \n",
    "    5. [Step 1 to 6: Build the chat steps](#Step1to6)\n",
    "    \n",
    " 3. [Chat with the assistant](#Chat%20with%20the%20assistant)\n",
    " \n",
    " 4. [Cleanup](#Cleanup)\n",
    " \n",
    " 5. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9fb9d3",
   "metadata": {},
   "source": [
    "##  1. Complete prerequisites <a id ='Complete%20prerequisites'> </a>\n",
    "\n",
    "Check and complete the prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85c39b",
   "metadata": {},
   "source": [
    "###  A. Check and configure access to the Internet <a id ='Check%20and%20configure%20access%20to%20the%20Internet'> </a>\n",
    "This notebook requires outbound access to the Internet to download the required software updates and to download the dataset.  You can either provide direct Internet access (default) or provide Internet access through an [Amazon VPC](https://aws.amazon.com/vpc/).  For more information on this, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/appendix-notebook-and-internet-access.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f0b44",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> During the AIM329 session, by default, outbound Internet access will be enabled for this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820efd56",
   "metadata": {},
   "source": [
    "### B. Check and upgrade required software versions <a id ='Check%20and%20upgrade%20required%20software%20versions'> </a>\n",
    "This notebook requires the following libraries:\n",
    "* [SageMaker Python SDK version 2.x](https://sagemaker.readthedocs.io/en/stable/v2.html)\n",
    "* [Python 3.10.x](https://www.python.org/downloads/release/python-3100/)\n",
    "* [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\n",
    "* [LangChain](https://www.langchain.com/)\n",
    "* [Unstructured](https://pypi.org/project/unstructured/)\n",
    "* [OpenSearch Python Client](https://pypi.org/project/opensearch-py/)\n",
    "* [AWS v4 authentication for the Python Requests library](https://pypi.org/project/requests-aws4auth/)\n",
    "\n",
    "Run the following cell to install the required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb373af",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Note:</b> At the end of the installation, the Kernel will be forcefully restarted immediately.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7256d4fc-0361-4cee-a548-d9b7e355824f",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3==1.28.72\n",
    "!pip install langchain==0.0.324\n",
    "!pip install unstructured==0.10.27\n",
    "!pip install opensearch-py==2.3.2\n",
    "!pip install requests-aws4auth==1.2.3\n",
    "\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ba26b",
   "metadata": {},
   "source": [
    "Print the versions of the installed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23f2d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import langchain\n",
    "import opensearchpy\n",
    "import sagemaker\n",
    "import sys\n",
    "\n",
    "print(\"Python version : {}\".format(sys.version))\n",
    "print(\"Boto3 version : {}\".format(boto3.__version__))\n",
    "print(\"SageMaker Python SDK version : {}\".format(sagemaker.__version__))\n",
    "print(\"LangChain version : {}\".format(langchain.__version__))\n",
    "print(\"OpenSearch Python Client version : {}\".format(opensearchpy.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3260b",
   "metadata": {},
   "source": [
    "###  C. Create an Amazon OpenSearch Serverless collection <a id ='Create%20an%20Amazon%20OpenSearch%20Serverless%20collection'> </a>\n",
    "\n",
    "This notebook uses an [Amazon OpenSearch Serverless collection](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-collections.html) as the vector database that will be used by the chat assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2687632",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> During the AIM329 session, by default, a collection will be pre-created and ready to use.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f0e97",
   "metadata": {},
   "source": [
    "###  D. Enable model access in Amazon Bedrock <a id ='Enable%20model%20access%20in%20Amazon%20Bedrock'> </a>\n",
    "\n",
    "Before invoking any model in Amazon Bedrock, enable access to that model by following the instructions [here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html). Otherwise, you will get an authorization error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57899f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Note:</b> You will have to do this manually after reading the End User License Agreement (EULA) for each of the models that you want to enable. Unless you disable it, this is a one-time setup for each model in an AWS account.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ee077",
   "metadata": {},
   "source": [
    "###  E. Check and configure security permissions <a id ='Check%20and%20configure%20security%20permissions'> </a>\n",
    "This notebook uses the IAM role attached to the underlying notebook instance.  To view the name of this role, run the following cell.\n",
    "\n",
    "This IAM role should have the following permissions,\n",
    "\n",
    "1. Full access to invoke Large Language Models (LLMs) on Amazon Bedrock.\n",
    "2. Full access to read and write to the Amazon OpenSearch Serverless collection created in the previous step.\n",
    "3. Access to write to Amazon CloudWatch Logs.\n",
    "\n",
    "In addition, [data access control](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-data-access.html) should be setup on the Amazon OpenSearch Serverless Collection to provide create, read and write access to the IAM role associated with this Notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f10781",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>  During the AIM329 session, by default, all these permissions will be setup.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c64186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sagemaker.get_execution_role())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3bb063",
   "metadata": {},
   "source": [
    "###  F. Organize imports <a id ='Organize%20imports'> </a>\n",
    "\n",
    "Organize all the library and module imports for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a06b9-812c-4dad-a652-1cb34aa9d8b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import requests\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.llms import Bedrock\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain, LLMChain, RetrievalQA\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "from langchain.retrievers import AmazonKendraRetriever\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from IPython.core.display import HTML\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "# Import the helper functions from the 'scripts' folder\n",
    "sys.path.append(os.path.join(os.getcwd(), \"scripts\"))\n",
    "#print(\"Updated sys.path: {}\".format(sys.path))\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12579ad7",
   "metadata": {},
   "source": [
    "###  G. Create common objects <a id='Create%20common%20objects'></a>\n",
    "\n",
    "To begin with, get the current AWS Region (where this notebook is running) and the SageMaker Session. This will be used to initiate some of the clients to AWS services using the boto3 APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91d265e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the AWS Region and SageMaker Session references\n",
    "my_session = boto3.session.Session()\n",
    "print(\"SageMaker Session: {}\".format(my_session))\n",
    "my_region = my_session.region_name\n",
    "print(\"AWS Region: {}\".format(my_region))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92db4b0a",
   "metadata": {},
   "source": [
    "Pick a LLM and the Embeddings model within Amazon Bedrock that you will be using in this notebook. Then, get their model-ids. In order to do this, list all the available models in Amazon Bedrock by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323e08e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List all the available models in Amazon Bedrock\n",
    "bedrock_client = boto3.client(\"bedrock\", region_name = my_region)\n",
    "response = bedrock_client.list_foundation_models()\n",
    "model_summaries = response[\"modelSummaries\"]\n",
    "for model_summary in model_summaries:\n",
    "    print(\"Model provider: {}; Model name: {}; Model-id: {}\".format(model_summary[\"providerName\"],\n",
    "                                                                    model_summary[\"modelName\"], model_summary[\"modelId\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a835e",
   "metadata": {},
   "source": [
    "Now, create common objects to be used in future steps in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb830d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Specify the model-ids\n",
    "# Model-id of the LLM to be used in the chat assistant\n",
    "llm_model_id = \"anthropic.claude-instant-v1\"\n",
    "# Model-id of the Embeddings model to be used in the chat assistant\n",
    "embeddings_model_id = \"amazon.titan-embed-text-v1\"\n",
    "\n",
    "##### LLM related objects\n",
    "# Create the Amazon Bedrock runtime client\n",
    "bedrock_rt_client = boto3.client(\"bedrock-runtime\", region_name = my_region)\n",
    "# Create the LangChain client for the LLM using the Bedrock client created above.\n",
    "llm = Bedrock(\n",
    "    model_id = llm_model_id,\n",
    "    client = bedrock_rt_client\n",
    ")\n",
    "\n",
    "##### Embeddings related objects\n",
    "# Use the LangChain BedrockEmbeddings class to create the Embeddings client.\n",
    "br_embeddings = BedrockEmbeddings(client = bedrock_rt_client, model_id = embeddings_model_id, region_name = my_region)\n",
    "\n",
    "##### Amazon OpenSearch Serverless (AOSS) related objects\n",
    "# Create the AOSS client\n",
    "aoss_client = boto3.client(\"opensearchserverless\")\n",
    "# Get the reference to the first Amazon OpenSearch Serverless (AOSS) collection within the same AWS Region\n",
    "response = aoss_client.list_collections()['collectionSummaries'][0]\n",
    "collection_id = response['id']\n",
    "collection_name = response['name']\n",
    "print(\"The following Amazon OpenSearch Serverless collection will be used:\\nCollection id: {}; Collection name: {}\"\n",
    "      .format(collection_id, collection_name))\n",
    "# Print the AWS console URL to the AOSS collection\n",
    "collection_aws_console_url = \"https://{}.console.aws.amazon.com/aos/home?region={}#opensearch/collections/{}\".format(my_region,\n",
    "                                                                                                                     my_region,\n",
    "                                                                                                                     collection_name)\n",
    "print(\"If you like to take a look at this collection, visit {}\".format(collection_aws_console_url))\n",
    "# Create the AOSS Python client from the AOSS boto3 client\n",
    "aoss_py_client = auth_opensearch(host = \"{}.{}.aoss.amazonaws.com\".format(collection_id, my_region),\n",
    "                            service = 'aoss', region = my_region)\n",
    "# Specify the name of the index in the AOSS collection; this will be created later in the notebook\n",
    "index_name = \"aim-329-docs-index\"\n",
    "# Specify the max workers for loading data in parallel into the index\n",
    "max_workers = 8\n",
    "# To access an Opensearch Collection using LangChain, we can use the OpenSearchVectorSearch class.\n",
    "doc_search = OpenSearchVectorSearch(\n",
    "    opensearch_url = \"{}.{}.aoss.amazonaws.com\".format(collection_id, my_region),\n",
    "    index_name = index_name,\n",
    "    embedding_function = br_embeddings)\n",
    "# Set the doc search client to the AOSS Python client\n",
    "doc_search.client = aoss_py_client\n",
    "\n",
    "##### File related objects\n",
    "# Specify the path to the directory that will contain the RAG data\n",
    "rag_dir = os.path.join(os.getcwd(), \"data/rag\")\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(rag_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc4b73",
   "metadata": {},
   "source": [
    "###  H. Create an index in the Amazon OpenSearch Serverless collection <a id='Create%20index%20in%20collection'></a>\n",
    "\n",
    "To create an index in the Amazon OpenSearch Serverless (AOSS) collection, we first need to define a schema for our index. AOSS allows users to specify a simple search index, which utilizes keyword matching, or the vector search feature, which utilizes [k-Nearest Neighbor (k-NN) search](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/knn.html). Vector search differs from standard search in that instead of using a typical keyword matching or fuzzy matching algorithm, vector search compares [embeddings](https://en.wikipedia.org/wiki/Word_embedding) of two pieces of text. An embedding is a numerical representation of a piece of information, like text, that we can compare against other embeddings. To learn more about embeddings, take a look at [this blog](https://huggingface.co/blog/getting-started-with-embeddings). The vector search feature allows us to search for documents that are semantically similar to the questions that our end users send to our chat assistant. This can improve the context that we then give to our LLM to answer the user's questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ff129-c6f3-484e-bf8c-15269e10f822",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the schema for the index with an k-NN type vector as the embedding\n",
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "           \"content-embedding\": { \n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1536 # can have dimension up to 10k\n",
    "            },\n",
    "            \"content\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"page-name\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"page-link\": {\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Delete the index\n",
    "#aoss_py_client.indices.delete(index = index_name)\n",
    "\n",
    "# Create the index if it does not exist\n",
    "if aoss_py_client.indices.exists(index = index_name):\n",
    "    print(\"AOSS index '{}' already exists.\".format(index_name))\n",
    "else:\n",
    "    print(\"Creating AOSS index '{}'...\".format(index_name))\n",
    "    pprint.pprint(aoss_py_client.indices.create(index = index_name, body = knn_index, ignore = 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e2d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the AWS console URL to the AOSS index\n",
    "#index_aws_console_url = collection_aws_console_url + \"?tabId=collectionIndices\"\n",
    "index_aws_console_url = collection_aws_console_url + \"/\" + index_name\n",
    "print(\"If you like to take a look at this index, visit {}\".format(index_aws_console_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f5d414",
   "metadata": {},
   "source": [
    "## 2. Build the chat assistant <a id ='Build%20the%20chat%20assistant'> </a>\n",
    "\n",
    "Large language models (LLMs) have a tendency to [hallucinate](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)). Hallucination in a LLM context is our model providing a confident but factually incorrect response that often tells us what the model thinks we want to hear, regardless of if it actually is the correct answer. One way to prevent LLMs from giving us incorrect information is by using a [Retrieval Augment Generation (RAG)](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html) mechanism.\n",
    "\n",
    "RAG allows us to provide our model with correct context information that it can use to ground its response in facts, instead of it trying to remember facts from its training data. To setup RAG, we need to have a document database that we can utilize to provide our model with related source documents. There are many ways to setup a document database. In this lab, we will use [Amazon Opensearch Serverless](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14eb789",
   "metadata": {},
   "source": [
    "###  A. Architecture <a id='Architecture'></a>\n",
    "\n",
    "![Architecture](./images/architecture.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aac8ba",
   "metadata": {},
   "source": [
    "###  B. Step 0a: Prepare to load data into the vector database <a id='Step0a'></a>\n",
    "\n",
    "An Amazon OpenSearch Serverless (AOSS) Collection is a logical grouping of one or more indexes. Run the following cell to populate some documentation on Amazon Bedrock so our chat assistant can answer questions about Amazon Bedrock with factually correct information.\n",
    "\n",
    "The following cell will download the data from the provided links to the Amazon Bedrock documentation and store them in a local folder named `data/aws_gen_ai/`. The directory will be created if it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e2feb9-af2d-4e95-8db0-23a5ef4e88b7",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A simple list of Amazon Bedrock documentation to index\n",
    "link_list = [\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-service.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/setting-up.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/text-playground.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/image-playground.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/embeddings.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-prepare.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-guidelines.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/agents.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html\",\n",
    "    \"https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html\",\n",
    "    \"https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html\",\n",
    "    \"https://docs.aws.amazon.com/lambda/latest/dg/welcome.html\",\n",
    "    \"https://docs.aws.amazon.com/lambda/latest/dg/getting-started.html\",\n",
    "    \"https://docs.aws.amazon.com/lambda/latest/dg/lambda-python.html\",\n",
    "    \"https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html\",\n",
    "    \"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-getting-started.html\",\n",
    "    \"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html\",\n",
    "    \"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.CreateTable.html\",\n",
    "]\n",
    "\n",
    "# Download and store each HTML file; overwrite existing file\n",
    "for link in link_list:\n",
    "    link_html = requests.get(link).content.decode('utf-8')\n",
    "    title = link.split(\"/\")[-1]\n",
    "    with open('{}/{}'.format(rag_dir, title), \"w\") as f:\n",
    "        f.write(link_html)\n",
    "\n",
    "print(\"Downloaded {} HTML files to '{}'.\".format(len(link_list), rag_dir))\n",
    "        \n",
    "# Display the last downloaded HTML file\n",
    "#HTML(link_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679ed1c8",
   "metadata": {},
   "source": [
    "When we are indexing documents for information retrieval, providing an entire document to a LLM as context can be overwhelming to our LLM, especially for very long documents. A best practice is to divide the document into easier to consume partially overlapping chunks. Dividing the document in this way also tends to improve search result relevance as often the answer we are looking for is contained within a specific passage of a document and providing the entire document is unnecessary. \n",
    "\n",
    "Let's use the LangChain's [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter) to create a text splitting object that we will use in our loading pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4187eb0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157d8a0",
   "metadata": {},
   "source": [
    "Now we have downloaded all of the documents, let's go ahead and load them using a HTML loader. We are going to use LangChain's [Unstructured HTML Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/html). This will parse the raw HTML documents we have from the Bedrock user guide and put it in a format that we can then give to our LLMs. Finally we will split each document as per the splitter configuration defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7537003-aebd-4d1a-911c-67e4d62c3f7c",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "# Load HTML using LangChain's UnstructuredHTMLLoader\n",
    "doc_data_list = []\n",
    "doc_link_list = []\n",
    "# Process every HTML file\n",
    "for link in link_list:\n",
    "    title = link.split(\"/\")[-1]\n",
    "    new_link = '{}/{}'.format(rag_dir, title)\n",
    "    \n",
    "    # Load the file content\n",
    "    loader = UnstructuredHTMLLoader(new_link)\n",
    "    data = loader.load()\n",
    "    \n",
    "    # Remove irrelevant text\n",
    "    html_doc = data[0].page_content.replace(\"\"\"Did this page help you? - Yes\n",
    "\n",
    "Thanks for letting us know we're doing a good job!\n",
    "\n",
    "If you've got a moment, please tell us what we did right so we can do more of it.\n",
    "\n",
    "Did this page help you? - No\n",
    "\n",
    "Thanks for letting us know this page needs work. We're sorry we let you down.\n",
    "\n",
    "If you've got a moment, please tell us how we can make the documentation better.\"\"\", \"\").replace(\"\"\"Javascript is disabled or is unavailable in your browser.\n",
    "\n",
    "To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\"\"\", \"\")\n",
    "    \n",
    "    # Split our document into chunks\n",
    "    texts = text_splitter.create_documents([html_doc])\n",
    "    \n",
    "    # Create a list of document chunks as well as a list of links\n",
    "    for text in texts:\n",
    "        doc_data_list.append(text.page_content)\n",
    "        doc_link_list.append(link)\n",
    "\n",
    "print(\"Created {} chunks from {} HTML files.\".format(len(doc_data_list), len(link_list)))        \n",
    "\n",
    "# Print the first chunk\n",
    "#print(doc_data_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12b335-5d79-45a7-b4d2-b487fefd6f64",
   "metadata": {},
   "source": [
    "###  C. Step 0b and 0c: Create the embeddings <a id='Step0band0c'></a>\n",
    "\n",
    "Now that we have the document data ready, let us vectorize it to create the embeddings by running the below cell which does the following,\n",
    "\n",
    "For each downloaded Amazon Bedrock documentation.\n",
    "1. Creates dictionary objects for the content of the document along with the name and link.\n",
    "2. Generates embeddings for the entire article and stores them in a dictionary object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449578e6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Note:</b> The below cell can take up to 3 minutes to complete.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e37392-7803-4c64-b074-3c525b6cb840",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through the chunked documents and create embeddings\n",
    "doc_list = []\n",
    "for i, doc_data in tqdm(enumerate(doc_data_list)):\n",
    "    # Get the data formatted for indexing; time.sleep in place to avoid throttling\n",
    "    doc_dict = {}\n",
    "    doc_dict['content'] = doc_data\n",
    "    doc_dict['page-name'] = doc_link_list[i].split('/')[-1].replace('.html','')\n",
    "    doc_dict['page-link'] = doc_link_list[i] \n",
    "    # Create the embedding for the document chunk\n",
    "    embedding = br_embeddings.embed_query(text = doc_data)\n",
    "    doc_dict['content-embedding'] = embedding\n",
    "    # Store all the data in a list\n",
    "    doc_list.append(doc_dict)\n",
    "    # Sleep for 2 seconds\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1801276",
   "metadata": {},
   "source": [
    "###  D. Step 0d: Store the embeddings in the vector database <a id='Step0d'></a>\n",
    "\n",
    "We have now created embeddings for all of our documents, so the next step is to actually upload them to our created AOSS index. The below function uses a parallel processing function to upload our documents into our index. The number of parallel worker threads is controlled by the `max_workers` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0d756",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> After executing the below cell, it may take up to 30 seconds for the data to be available for reading.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d765149",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Populate the AOSS index. OpenSearch has a flexible schema, so you can add fields you have not previously defined,\n",
    "# with the exception of vectors and fields with specific data types like dates.\n",
    "def os_import(article):\n",
    "    \"\"\"\n",
    "    This function imports the documents and their metadata into the AOSS index.\n",
    "    \"\"\"\n",
    "    aoss_py_client.index(index = index_name,\n",
    "                         body={\"content-embedding\": article['content-embedding'],\n",
    "                               \"content\": article['content'],\n",
    "                               \"page-name\": article['page-name'],\n",
    "                               \"page-link\": article['page-link'],\n",
    "                              }\n",
    "                        )\n",
    "    \n",
    "# Parallelize and populate the AOSS Collection's index\n",
    "process_map(os_import, doc_list, max_workers = max_workers) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa1860",
   "metadata": {},
   "source": [
    "###  E. Step 1 to 6: Build the chat steps <a id='Step1to6'></a>\n",
    "\n",
    "In order to demonstrate the usefulness of the [Retrieval Augment Generation (RAG)](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html) architecture, let us directly call the LLM without any searches on the vector database. Here, we will ask a question about Amazon Bedrock Agents which we know the LLM will not be aware of because Bedrock Agents was not available when the LLM was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2af9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = llm.generate([\"How do I setup Agents in Amazon Bedrock?\"])\n",
    "print(output.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fd2117",
   "metadata": {},
   "source": [
    "While this response may seem plausible, it is actually incorrect. In general, LLMs will try to answer your question but in this case it has hallucinated. This is an example of how the LLM was not able to provide the correct answer. Here is where a RAG architecture will provide the remedy.\n",
    "\n",
    "Let us see if we can find a document that contains information about Amazon Bedrock Agents in our document index. We can do this in two ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e32b5",
   "metadata": {},
   "source": [
    "Method 1: Using the AOSS Python client's `search` directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3043bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Amazon Bedrock?\"\n",
    "temp_embedding = br_embeddings.embed_query(text = query)\n",
    "search_query = {\"query\": {\"knn\": {\"content-embedding\": {\"vector\": temp_embedding, \"k\": 4}}}}\n",
    "results = aoss_py_client.search(index = index_name, body = search_query)\n",
    "hits = results[\"hits\"][\"hits\"]\n",
    "print(\"Found {} hit(s).\".format(len(hits)))\n",
    "for hit in hits:\n",
    "    print(hit[\"_source\"][\"page-link\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32757e9e",
   "metadata": {},
   "source": [
    "Method 2: Using LangChain's `similarity_search` which will use the AOSS Python client under the covers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f6564",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_results = 5\n",
    "query = \"How do I setup Agents in Amazon Bedrock?\"\n",
    "docs = doc_search.similarity_search(\n",
    "    # Our text query\n",
    "    query = query,\n",
    "    # The name of the field that contains our vector\n",
    "    vector_field = \"content-embedding\",\n",
    "    # The actual text field we are looking for\n",
    "    text_field = \"content\",\n",
    "    # The number of results we want to return\n",
    "    k = max_results\n",
    ")\n",
    "print(\"Specified {} max results. Found {} hit(s).\".format(max_results, len(docs)))\n",
    "for doc in docs:\n",
    "    print(docs[0].metadata['page-link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ea8ad",
   "metadata": {},
   "source": [
    "It looks like we do have some information on Amazon Bedrock Agents in the Bedrock user guide that we vectorized and stored in the AOSS index. Now that we know we have the right information in our document index, let us setup a [RetrievalQA chain](https://python.langchain.com/docs/use_cases/question_answering/vector_db_qa). This chain allows us to supply a [prompt template](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/), our LLM, and our document index to form a question answering chain that will answer questions based on the returned context document. \n",
    "\n",
    "Prompt templates are pre-defined recipes for generating prompts for language models. In the one we create below, we specify context and question input variables, which our RetrievalQA chain will fill in with the query and source documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad37629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the prompt template\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Don't include harmful content.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, input_variables = [\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the Retrieval QA chain\n",
    "qa = RetrievalQA.from_chain_type(llm = llm, \n",
    "                                 chain_type = \"stuff\", \n",
    "                                 retriever = doc_search.as_retriever(search_kwargs = {\n",
    "                                     \"vector_field\": \"content-embedding\",\n",
    "                                     \"text_field\": \"content\",\n",
    "                                     \"k\": 5}),\n",
    "                                 return_source_documents = True,\n",
    "                                 chain_type_kwargs = {\"prompt\": PROMPT, \"verbose\": True},\n",
    "                                 verbose = True)\n",
    "\n",
    "# Ask the question to the LLM and print the response along with the references from the source\n",
    "question = \"How do I setup Agents in Amazon Bedrock?\"\n",
    "response = qa(question, return_only_outputs = True)\n",
    "print(\"Answer:\\n{}\".format(response[\"result\"]))\n",
    "source_metadata = response[\"source_documents\"][0].metadata\n",
    "print(\"\\n\\nSource page name: {}\".format(source_metadata[\"page-name\"]))\n",
    "print(\"Source page link: {}\".format(source_metadata[\"page-link\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9202e239-b6e8-4252-9f8d-b81c499478a9",
   "metadata": {},
   "source": [
    "Let's now take it one step further with a [ConversationalRetrievalChain](https://python.langchain.com/docs/expression_language/cookbook/retrieval#conversational-retrieval-chain). \n",
    "\n",
    "LLMs on their own will not remember the last input you provided them. So we need a mechanism to remember and supply our previous conversation information back to our LLM. We can do this using a LangChain ConversationChain, paired with a [ConversationBufferMemory](https://python.langchain.com/docs/modules/memory/adding_memory) class. This will allow us to hold a conversation with our LLM and retain the previous conversation in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c777a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm = llm, verbose = True, memory = ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae508cb9",
   "metadata": {},
   "source": [
    "The following cell adds a conversational element to the retrieval chain and allows us to add chat memory to the retrieval. This chain uses a LLM call prior to the document retrieval that condenses conversation history and the current question into a single new question to improve document retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71e061-4ea4-45a5-ac76-c78945942681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the Conversational Retrieval chain\n",
    "cqa = ConversationalRetrievalChain.from_llm(llm = llm, \n",
    "                                            chain_type = \"stuff\", \n",
    "                                            condense_question_llm = llm,\n",
    "                                            retriever = doc_search.as_retriever(search_kwargs = {\n",
    "                                                \"vector_field\": \"content-embedding\",\n",
    "                                                \"text_field\": \"content\",\n",
    "                                                \"k\": 5}),\n",
    "                                            return_source_documents = True,\n",
    "                                            memory = ConversationBufferMemory(input_key = \"question\", output_key = \"answer\"),\n",
    "                                            # chain_type_kwargs={\"prompt\": PROMPT, \"verbose\": True},\n",
    "                                            verbose = False)\n",
    "\n",
    "# Ask the question to the LLM and print the response along with the references from the source\n",
    "question = \"How do I setup Agents in Amazon Bedrock?\"\n",
    "response = cqa.invoke({\"question\": question, \"chat_history\": \"\"})\n",
    "print(\"Answer:\\n{}\".format(response[\"answer\"]))\n",
    "source_metadata = response[\"source_documents\"][0].metadata\n",
    "print(\"\\n\\nSource page name: {}\".format(source_metadata[\"page-name\"]))\n",
    "print(\"Source page link: {}\".format(source_metadata[\"page-link\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d6d33-e76c-483c-9fc8-f84f3e81e273",
   "metadata": {},
   "source": [
    "## 3. Chat with the assistant <a id='Chat%20with%20the%20assistant'></a>\n",
    "\n",
    "Now we are going to put it all together in a single browser interface. The below call will initiate a simple conversational UI inside of our Jupyter Notebook. Run it and start asking questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b148c66-6bce-4fb4-a784-b51907912cf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chatux = ChatUX(cqa)\n",
    "chatux.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1216a",
   "metadata": {},
   "source": [
    "## 4. Cleanup <a id='Cleanup'></a>\n",
    "\n",
    "As a best practice, you should delete AWS resources that are no longer required.  This will help you avoid incurring unncessary costs.\n",
    "\n",
    "**Note:** During the AIM329 session, by default, all resources will be cleaned up at the end of the session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bd52a5",
   "metadata": {},
   "source": [
    "## 5. Conclusion <a id='Conclusion'></a>\n",
    "\n",
    "We have now seen how to build chat assistant using a Large Language Model (LLM) hosted on Amazon Bedrock. In the process, we also demonstrated how a Retrieval Augment Generation (RAG) mechanism can help prevent hallucination. While using RAG, we showed you how to use Amazon Titan Embeddings to convert raw text to vectors and how to store them in an Amazon OpenSearch Serverless collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9731e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
